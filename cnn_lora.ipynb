{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f58caf-4fb5-4070-a146-e693c49c9633",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook will attempt to adapt an existing CNN model to use LoRA for finetuning. \n",
    "\n",
    "Model choice - [MobileNetV2 trained on ImageNet1k](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.MobileNet_V2_Weights) Reason: smallest classification CNN available via torchvision (by number of parameters), which means I can fine tune relatively faster, and there shouldn't be anything fundementally different with larger models.\n",
    "\n",
    "Original Dataset - ImageNet1k_V2\n",
    "\n",
    "Finetuning Dataset - [FGVCAIRCRAFT](https://pytorch.org/vision/stable/generated/torchvision.datasets.FGVCAircraft.html#torchvision.datasets.FGVCAircraft) (The dataset contains 10,000 images of aircrafts across 30 different manufacturers) Reason: Relatively small dataset (thus faster training and more representative of realworld finetuning task) and looking at airplanes is fun!\n",
    "\n",
    "Ideas: try varying the amount of training data? how well does finetuning work on small data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84cb5ca6-e596-4b7c-9987-e6cf6fb8e323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import FGVCAircraft\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import loralib as lora\n",
    "\n",
    "import os\n",
    "\n",
    "# make results reproducible\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a6478762-f942-4253-886d-e1a27d79603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASETS = '/home/sunil/cnn-lora/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b33503b-4a8f-4cd7-b3d6-81c729278e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCNN(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, lora_on=False, data_dir=PATH_DATASETS):\n",
    "        '''\n",
    "        :lora: flag - if true uses LoRA\n",
    "        :data_dir: - string path to where to store data\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # some hyperparameters\n",
    "        self.lr = 1e-4\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        # dataset specific information\n",
    "        self.data_dir = data_dir\n",
    "        self.num_classes = 30 # there are 30 manufacturers in our dataset \n",
    "        \n",
    "        # define lore hyper params\n",
    "        self.lora_on = lora_on\n",
    "        lora_rank = 8\n",
    "\n",
    "        # define the model\n",
    "        self.model = torchvision.models.mobilenet_v2()\n",
    "\n",
    "        # reset the mlp head regardless of if we are using LoRA or not \n",
    "        self.model.classifier[1] = nn.Linear(1280, self.num_classes)\n",
    "        nn.init.normal_(self.model.classifier[1].weight, 0, 0.01)\n",
    "        nn.init.zeros_(self.model.classifier[1].bias)\n",
    "\n",
    "        # setup model for lora if desired\n",
    "        if self.lora_on:\n",
    "            # replace all conv layers with lora conv layers\n",
    "            for name, module in self.model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    in_channels = module.in_channels\n",
    "                    out_channels = module.out_channels\n",
    "                    kernel_size = module.kernel_size[0]\n",
    "                    stride = module.stride\n",
    "                    padding = module.padding\n",
    "                    dialtion = module.dilation\n",
    "                    groups = module.groups\n",
    "                    bias = module.bias if module.bias is not None else False\n",
    "                    padding_mode = module.padding_mode\n",
    "\n",
    "                    new_conv = lora.Conv2d(in_channels=in_channels, \n",
    "                                         out_channels=out_channels,\n",
    "                                         kernel_size=kernel_size,\n",
    "                                         stride=stride,\n",
    "                                         padding=padding,\n",
    "                                         dilation=dialtion,\n",
    "                                         groups=groups,\n",
    "                                         bias=bias,\n",
    "                                         padding_mode=padding_mode,\n",
    "                                         r=lora_rank\n",
    "                                         )\n",
    "\n",
    "\n",
    "                    # `parts` is essentially a list of keys we can use to determine where this layer belongs\n",
    "                    parts = name.split('.')\n",
    "                    self.set_model_feature(parts, new_conv)\n",
    "\n",
    "            # setup layer freezes properly for lora\n",
    "            # TODO is this fixing our new mlp head too?\n",
    "            lora.mark_only_lora_as_trainable(self.model)\n",
    "            \n",
    "            # load in pretrained weights\n",
    "            # TODO: remove mlp head weights from the state dict\n",
    "            self.model.load_state_dict(torch.load(self.get_initial_state_dict_path()), strict=False)\n",
    "            \n",
    "        \n",
    "            \n",
    "        # define metrics\n",
    "        self.val_accuray = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        \n",
    "    # helper function to update layers with lora layers during init\n",
    "    def set_model_feature(self, keys, value):\n",
    "        pointer = self.model\n",
    "        while len(keys) > 1:\n",
    "            key = keys.pop(0)\n",
    "            pointer = getattr(pointer, key)\n",
    "\n",
    "        key = keys.pop(0)\n",
    "        pointer[int(key)] = value\n",
    "\n",
    "    def get_initial_state_dict_path(self):\n",
    "        '''\n",
    "        saves the imagenet 1k weights to a state dict on disk (if necessary) and returns path to where they are saved\n",
    "        '''\n",
    "        weights_path = 'mobilenet_v2_imagenet1k_state_dict.pt'\n",
    "\n",
    "        if not os.path.isfile(weights_path):\n",
    "            m = torchvision.models.mobilenet_v2(weights=torchvision.models.mobilenetv2.MobileNet_V2_Weights.DEFAULT)\n",
    "            torch.save(m.state_dict(), weights_path)\n",
    "\n",
    "        return weights_path\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: any transforms?\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x,1)\n",
    "\n",
    "    # centralize stuff we need in train/val/test\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return x, y, logits, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, _, _, loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, y, logits, loss = self.common_step(batch, batch_idx)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, y, logits, loss = self.common_step(batch, batch_idx)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_accuracy.update(preds, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download data and set transforms\n",
    "\n",
    "        #TODO: transforms!\n",
    "        FGVCAircraft(self.data_dir, split='train', annotation_level='manufacturer', download=True) \n",
    "        FGVCAircraft(self.data_dir, split='val', annotation_level='manufacturer', download=True)\n",
    "        FGVCAircraft(self.data_dir, split='test', annotation_level='manufacturer', download=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.aircraft_train = FGVCAircraft(self.data_dir, split='train', annotation_level='manufacturer')\n",
    "            self.aircraft_val = FGVCAircraft(self.data_dir, split='val', annotation_level='manufacturer')\n",
    "            \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.aircraft_test = FGVCAircraft(self.data_dir, split='test', annotation_level='manufacturer')\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(self.aircraft_train, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(self.aircraft_val, batch_size=self.batch_size, num_workers=16)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(self.aircraft_test, batch_size=self.batch_size, num_workers=16)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "65c50bc5-0e50-43a4-97ac-95ad96ddb8ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNetV2:\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([1000, 1280]) from checkpoint, the shape in current model is torch.Size([30, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([30]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mLitCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmodel\n",
      "Cell \u001b[0;32mIn[150], line 67\u001b[0m, in \u001b[0;36mLitCNN.__init__\u001b[0;34m(self, lora_on, data_dir)\u001b[0m\n\u001b[1;32m     64\u001b[0m     lora\u001b[38;5;241m.\u001b[39mmark_only_lora_as_trainable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# load in pretrained weights\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_initial_state_dict_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# define metrics\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_accuray \u001b[38;5;241m=\u001b[39m Accuracy(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn-lora/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNetV2:\n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([1000, 1280]) from checkpoint, the shape in current model is torch.Size([30, 1280]).\n\tsize mismatch for classifier.1.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([30])."
     ]
    }
   ],
   "source": [
    "m = LitCNN(lora_on=True)\n",
    "model = m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afb93c-f36e-4a9c-9a34-0e3776fe3e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
